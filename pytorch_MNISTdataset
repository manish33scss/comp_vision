"""
Created on Sun May 24 23:44:24 2020

@author: manish kumar
"""

import torch.nn as nn #initialize
import torch.nn.functional as F 
import torch
import torchvision
from torchvision import transforms,datasets
import matplotlib.pyplot as plt

train = datasets.MNIST("", train = True, download = True, transform= transforms.Compose([transforms.ToTensor()]))


test= datasets.MNIST('', train=False,download=True,transform=transforms.Compose([transforms.ToTensor()]))


trainset=torch.utils.data.DataLoader(train,batch_size=10,shuffle=True)
testset=torch.utils.data.DataLoader(test,batch_size=10,shuffle=True)


for data in trainset:
    print(data)
    break


x,y=data[0][0],data[0][1]
plt.imshow(data[0][0].view(28,28))
plt.show()

#to balance dataset

total=0
counter_d= {0:0, 1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0}

for data in trainset:
    Xs, ys= data
    for y in ys:
        counter_d[int(y)]+=1
        total+=1
print(counter_d)


#percentage distribution

for i in counter_d:
    print(f"{i} : {counter_d[i]/total*100}")
    

class Net(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.fc1 = nn.Linear(28*28 , 64) #we need to pass flattened image !! (28*28) and output is 64
        self.fc2 = nn.Linear(64 , 64)
        self.fc3 = nn.Linear(64 , 64)
        self.fc4 = nn.Linear(64 , 10) #output layer
        
    #feed forward
    def forward(self,x):
        x= F.relu(self.fc1(x)) #activation func runs on o/p i.e. 64 in this case
    
        x= F.relu(self.fc2(x))
        x= F.relu(self.fc3(x)) 
        x= self.fc4(x) #so we dont need to use activation func here #we need prob distribution so softmax is used.
        
        
        return F.log_softmax(x,dim=1) #distributing across the op layer tensors  dim 1
    
net= Net()

print(net)



X= torch.rand((28,28))    
X=X.view(1,28*28)    

import torch.optim as optim

optimizer= optim.Adam(net.parameters(), lr=0.001 )   #kind_hyperparameter #size of step the optimizer takes to get to optimized value We use decaying learning rate where step size decreases gradually

#iterate all over the data nd feed to network

EPOCH= 3
for epoch in range(EPOCH):
    # data is batch featuresets and labels
    for data in trainset:
        X,y=data
        net.zero_grad()
        output= net(X.view(-1,28*28))
        loss= F.nll_loss(output,y)
        loss.backward() #backpropogate
        optimizer.step()
    print(loss)
    
    
correct=0
total=0

with torch.no_grad():
    for data in trainset:
        X,y=data
        output= net(X.view(-1,784))
        for idx,i in enumerate(output):
            if torch.argmax(i)== y[idx]:
                correct+=1
            total += 1
print("Acc", round(correct/total,3))

plt.imshow(X[2].view(28,28))

#crosscheck
print(torch.argmax(net(X[2].view(-1,784))[0]))
  
